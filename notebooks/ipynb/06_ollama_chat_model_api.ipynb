{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa0f65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp chat_model/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf67c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from collections.abc import Callable, Iterator\n",
    "\n",
    "from ollama import ChatResponse as OllamaChatResponse\n",
    "from ollama import Client\n",
    "\n",
    "from liquidagent.chat_model.base import ChatModelProtocol, ChatModelResponse, Tool\n",
    "from liquidagent.config import OllamaProviderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d14b97",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Adding the implementation for Ollama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a060eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Ollama chat model\n",
    "class OllamaChatModel(ChatModelProtocol):\n",
    "    def __init__(self, config: OllamaProviderConfig) -> None:\n",
    "        self.client = Client()\n",
    "        self.config = config\n",
    "\n",
    "    def _convert_messages(self, messages: list[str]) -> list[str]:\n",
    "        return messages\n",
    "\n",
    "    def _convert_resp(self, resp: OllamaChatResponse) -> ChatModelResponse:\n",
    "        return ChatModelResponse.model_validate(resp, from_attributes=True)\n",
    "\n",
    "    def bind_tools(self, tools: list[Tool | Callable]) -> None:\n",
    "        self.tools = tools\n",
    "\n",
    "    def invoke(\n",
    "        self, messages=[], stream=False\n",
    "    ) -> ChatModelResponse | Iterator[ChatModelResponse]:\n",
    "        messages = self._convert_messages(messages)\n",
    "        resp = self.client.chat(\n",
    "            model=self.config.model, messages=messages, stream=stream, tools=self.tools\n",
    "        )\n",
    "        if not stream:\n",
    "            resp = self._convert_resp(resp)\n",
    "            return resp\n",
    "        else:\n",
    "            return map(lambda chunk: self._convert_resp(chunk), resp)\n",
    "\n",
    "    def agent(\n",
    "        self, messages=[], stream=False, available_functions: dict[str, Callable] = {}\n",
    "    ) -> ChatModelResponse | Iterator[ChatModelResponse]:\n",
    "        response = self.invoke(messages, stream)\n",
    "        if stream:\n",
    "            for chunk in response:\n",
    "                if chunk.message.tool_calls:\n",
    "                    # There may be multiple tool calls in the response\n",
    "                    for tool in chunk.message.tool_calls:\n",
    "                        # Ensure the function is available, and then call it\n",
    "                        if function_to_call := available_functions.get(\n",
    "                            tool.function.name\n",
    "                        ):\n",
    "                            print(\"Calling function:\", tool.function.name)\n",
    "                            print(\"Arguments:\", tool.function.arguments)\n",
    "                            output = function_to_call(**tool.function.arguments)\n",
    "                            print(\"Function output:\", output)\n",
    "                        else:\n",
    "                            print(\"Function\", tool.function.name, \"not found\")\n",
    "\n",
    "                        # Add the function response to messages for the model to use\n",
    "                        messages.append(chunk.message)\n",
    "                        messages.append(\n",
    "                            {\n",
    "                                \"role\": \"tool\",\n",
    "                                \"content\": str(output),\n",
    "                                \"name\": tool.function.name,\n",
    "                            }\n",
    "                        )\n",
    "                        self.agent(messages, stream, available_functions)\n",
    "                else:\n",
    "                    print(chunk.message.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0f2b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev  # noqa\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812984f-cea5-4a26-8911-9f7495064270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
